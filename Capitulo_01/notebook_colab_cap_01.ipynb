# Capítulo 01 – Entendendo os LLMs
Este notebook explora os conceitos fundamentais apresentados no Capítulo 01 do livro *Build a Large Language Model (From Scratch)*.

## 1.1 O que é um LLM?
- LLMs são modelos de rede neural profunda que geram texto previsível com base em grandes conjuntos de dados.
- Utilizam arquitetura Transformer e aprendizado auto-supervisionado.
- São modelos generativos (GenAI), como o ChatGPT.

## 1.2 Aplicações dos LLMs
- Tradução, resumo, geração de texto, criação de código, assistentes virtuais e muito mais.
- Capacidade de responder perguntas complexas e interagir em linguagem natural.

## 1.3 Etapas da Construção de um LLM
- **Pretraining**: modelo aprende padrões gerais da linguagem com dados não rotulados.
- **Fine-tuning**: modelo é ajustado para tarefas específicas com dados rotulados.
- Vantagens dos LLMs personalizados: privacidade, performance local e flexibilidade.

## 1.4 Arquitetura Transformer
- Composta por Encoder e Decoder (GPT usa apenas o Decoder).
- Mecanismo de *self-attention* permite o modelo focar em partes relevantes do texto.
- GPT é autoregressivo, gerando uma palavra por vez.

## 1.5 Utilização de Grandes Datasets
- GPT-3 treinado com ~300 bilhões de tokens (WebText2, CommonCrawl, Wikipedia, Books).
- Alto custo computacional (milhões de dólares).
- Alternativas open-source e datasets públicos são opções viáveis para estudos.

## 1.6 Arquitetura do GPT
- GPT é baseado somente no Decoder do Transformer.
- Treinado com tarefa de *next-word prediction*.
- Possui propriedades emergentes como tradução e zero/few-shot learning.

## 1.7 Construindo um LLM
- Três etapas principais:
  1. Preparação dos dados + mecanismo de atenção
  2. Pretraining (modelo base)
  3. Fine-tuning (tarefa específica)
- Implementações serão abordadas nos próximos capítulos.

# Exemplo simples de geração de sequência
sequencia = ['O', 'modelo', 'está', 'funcionando']
sequencia.append('bem!')
print(' '.join(sequencia))
