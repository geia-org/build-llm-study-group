{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "db40b004",
      "metadata": {
        "id": "db40b004"
      },
      "source": [
        "# Capítulo 01 – Entendendo os LLMs\n",
        "Este notebook explora os conceitos fundamentais apresentados no Capítulo 01 do livro *Build a Large Language Model (From Scratch)*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ae059d",
      "metadata": {
        "id": "c1ae059d"
      },
      "source": [
        "## 1.1 O que é um LLM?\n",
        "- LLMs são modelos de rede neural profunda que geram texto previsível com base em grandes conjuntos de dados.\n",
        "- Utilizam arquitetura Transformer e aprendizado auto-supervisionado.\n",
        "- São modelos generativos (GenAI), como o ChatGPT."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e19d2f96",
      "metadata": {
        "id": "e19d2f96"
      },
      "source": [
        "## 1.2 Aplicações dos LLMs\n",
        "- Tradução, resumo, geração de texto, criação de código, assistentes virtuais e muito mais.\n",
        "- Capacidade de responder perguntas complexas e interagir em linguagem natural."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4111f8a",
      "metadata": {
        "id": "e4111f8a"
      },
      "source": [
        "## 1.3 Etapas da Construção de um LLM\n",
        "- **Pretraining**: modelo aprende padrões gerais da linguagem com dados não rotulados.\n",
        "- **Fine-tuning**: modelo é ajustado para tarefas específicas com dados rotulados.\n",
        "- Vantagens dos LLMs personalizados: privacidade, performance local e flexibilidade."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031eeae7",
      "metadata": {
        "id": "031eeae7"
      },
      "source": [
        "## 1.4 Arquitetura Transformer\n",
        "- Composta por Encoder e Decoder (GPT usa apenas o Decoder).\n",
        "- Mecanismo de *self-attention* permite o modelo focar em partes relevantes do texto.\n",
        "- GPT é autoregressivo, gerando uma palavra por vez."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e390acb0",
      "metadata": {
        "id": "e390acb0"
      },
      "source": [
        "## 1.5 Utilização de Grandes Datasets\n",
        "- GPT-3 treinado com ~300 bilhões de tokens (WebText2, CommonCrawl, Wikipedia, Books).\n",
        "- Alto custo computacional (milhões de dólares).\n",
        "- Alternativas open-source e datasets públicos são opções viáveis para estudos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f697bce4",
      "metadata": {
        "id": "f697bce4"
      },
      "source": [
        "## 1.6 Arquitetura do GPT\n",
        "- GPT é baseado somente no Decoder do Transformer.\n",
        "- Treinado com tarefa de *next-word prediction*.\n",
        "- Possui propriedades emergentes como tradução e zero/few-shot learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "805e9fdc",
      "metadata": {
        "id": "805e9fdc"
      },
      "source": [
        "## 1.7 Construindo um LLM\n",
        "- Três etapas principais:\n",
        "  1. Preparação dos dados + mecanismo de atenção\n",
        "  2. Pretraining (modelo base)\n",
        "  3. Fine-tuning (tarefa específica)\n",
        "- Implementações serão abordadas nos próximos capítulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b94d9d",
      "metadata": {
        "id": "a1b94d9d"
      },
      "outputs": [],
      "source": [
        "# Exemplo simples de geração de sequência\n",
        "sequencia = ['O', 'modelo', 'está', 'funcionando']\n",
        "sequencia.append('bem!')\n",
        "print(' '.join(sequencia))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}