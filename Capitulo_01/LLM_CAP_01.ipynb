{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "db40b004",
      "metadata": {
        "id": "db40b004"
      },
      "source": [
        "# Capítulo 01 – Entendendo os LLMs\n",
        "Este notebook explora os conceitos fundamentais apresentados no Capítulo 01 do livro *Build a Large Language Model (From Scratch)*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ae059d",
      "metadata": {
        "id": "c1ae059d"
      },
      "source": [
        "## 1.1 O que é um LLM?\n",
        "- LLMs são modelos de rede neural profunda que geram texto previsível com base em grandes conjuntos de dados.\n",
        "- Utilizam arquitetura Transformer e aprendizado auto-supervisionado.\n",
        "- São modelos generativos (GenAI), como o ChatGPT."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e19d2f96",
      "metadata": {
        "id": "e19d2f96"
      },
      "source": [
        "## 1.2 Aplicações dos LLMs\n",
        "- Tradução, resumo, geração de texto, criação de código, assistentes virtuais e muito mais.\n",
        "- Capacidade de responder perguntas complexas e interagir em linguagem natural."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4111f8a",
      "metadata": {
        "id": "e4111f8a"
      },
      "source": [
        "## 1.3 Etapas da Construção de um LLM\n",
        "- **Pretraining**: modelo aprende padrões gerais da linguagem com dados não rotulados.\n",
        "- **Fine-tuning**: modelo é ajustado para tarefas específicas com dados rotulados.\n",
        "- Vantagens dos LLMs personalizados: privacidade, performance local e flexibilidade."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031eeae7",
      "metadata": {
        "id": "031eeae7"
      },
      "source": [
        "## 1.4 Arquitetura Transformer\n",
        "- Composta por Encoder e Decoder (GPT usa apenas o Decoder).\n",
        "- Mecanismo de *self-attention* permite o modelo focar em partes relevantes do texto.\n",
        "- GPT é autoregressivo, gerando uma palavra por vez."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e390acb0",
      "metadata": {
        "id": "e390acb0"
      },
      "source": [
        "## 1.5 Utilização de Grandes Datasets\n",
        "- GPT-3 treinado com ~300 bilhões de tokens (WebText2, CommonCrawl, Wikipedia, Books).\n",
        "- Alto custo computacional (milhões de dólares).\n",
        "- Alternativas open-source e datasets públicos são opções viáveis para estudos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f697bce4",
      "metadata": {
        "id": "f697bce4"
      },
      "source": [
        "## 1.6 Arquitetura do GPT\n",
        "- GPT é baseado somente no Decoder do Transformer.\n",
        "- Treinado com tarefa de *next-word prediction*.\n",
        "- Possui propriedades emergentes como tradução e zero/few-shot learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "805e9fdc",
      "metadata": {
        "id": "805e9fdc"
      },
      "source": [
        "## 1.7 Construindo um LLM\n",
        "- Três etapas principais:\n",
        "  1. Preparação dos dados + mecanismo de atenção\n",
        "  2. Pretraining (modelo base)\n",
        "  3. Fine-tuning (tarefa específica)\n",
        "- Implementações serão abordadas nos próximos capítulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a1b94d9d",
      "metadata": {
        "id": "a1b94d9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O modelo está funcionando bem!\n"
          ]
        }
      ],
      "source": [
        "# Exemplo simples de geração de sequência\n",
        "sequencia = ['O', 'modelo', 'está', 'funcionando']\n",
        "sequencia.append('bem!')\n",
        "print(' '.join(sequencia))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2849e41",
      "metadata": {},
      "source": [
        "## Usando um LLM via Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "55264426",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The book is on Kindle here.\\n\\nI wish I would have read that earlier because I would have found it fascinating. But instead, I found it a bit too light and long for my tastes, for me. And I hate being a reader'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# exemplo de uso de um modelo GPT disponível no Hugging Face\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"GPT2\")\n",
        "response = pipe(\"The book is on\")\n",
        "response[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "84f8583d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the book is on the internet\n",
            "the book is on the.\n",
            "the book is on the shelf\n",
            "the book is on the list\n",
            "the book is on the map\n"
          ]
        }
      ],
      "source": [
        "# exemplo de uso de um modelo do tipo BERT\n",
        "from transformers import pipeline\n",
        "pipe = pipeline('fill-mask', model=\"distilbert/distilbert-base-uncased\")\n",
        "response = pipe(\"The book is on the [MASK]\")\n",
        "for item in response:\n",
        "    print(item[\"sequence\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87c2068f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_0', 'score': 3.795095920562744}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# exemplo de um modelo BERT ajustado (fine-tunning), que preve a quantidade de Story Points a partir do texto da User Story\n",
        "from transformers import pipeline\n",
        "pipe = pipeline('text-classification', model=\"giseldo/distilbert_bert_uncased_finetuned_story_point\")\n",
        "response = pipe(\"I want register a product with name and price\")\n",
        "response"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
